# AI Orchestrator Rules for Cursor

# This file instructs Cursor on how to use the AI Orchestrator MCP server
# for intelligent multi-model task routing and complete project development.

## Overview

You have access to an AI Orchestrator that provides:

### Multi-Model Routing
- **ChatGPT (OpenAI)**: Architecture, system design, roadmaps, planning
- **Claude (Anthropic)**: Coding, implementation, debugging, technical writing
- **Gemini (Google)**: Reasoning, analysis, explanations, research
- **Kimi (Moonshot)**: Code review, quality assessment, best practices

### Project Execution & Auto-Fix
- **run_project**: Execute any project with auto-detection
- **test_project**: Run tests with framework auto-detection
- **analyze_errors**: Deep error analysis with root cause
- **fix_issues**: AI-powered auto-fix with validation
- **verify_project**: Complete run-test-fix loop

---

## Complete Tool Reference

### Task Orchestration Tools

| Tool | Purpose | Example |
|------|---------|---------|
| `orchestrate_task` | Full multi-model execution | `orchestrate_task("Build a REST API")` |
| `analyze_task` | See routing without execution | `analyze_task("Create auth system")` |
| `route_to_model` | Direct model routing | `route_to_model("Fix bug", "anthropic")` |
| `check_status` | Check available models | `check_status()` |
| `get_available_models` | List all models | `get_available_models()` |

### Execution & Testing Tools

| Tool | Purpose | Example |
|------|---------|---------|
| `run_project` | Execute project | `run_project("/path/to/project")` |
| `test_project` | Run test suite | `test_project("/path/to/project")` |
| `analyze_errors` | Analyze errors | `analyze_errors("/path/to/project")` |
| `fix_issues` | Apply auto-fixes | `fix_issues("/path/to/project")` |
| `verify_project` | Full verification loop | `verify_project("/path/to/project")` |
| `orchestrate_full_development` | Complete workflow | `orchestrate_full_development(description, path)` |

---

## When to Use Each Model

### Use ChatGPT (via `route_to_model(task, "openai")`) for:
- System architecture design
- API design and specifications
- Project roadmaps and planning
- Database schema design
- Microservices architecture
- Technology stack decisions
- High-level design documents

### Use Claude (via `route_to_model(task, "anthropic")`) for:
- Writing new code and functions
- Debugging and fixing errors
- Code refactoring and optimization
- Implementing features from specs
- Writing tests
- Technical documentation
- Code explanations

### Use Gemini (via `route_to_model(task, "gemini")`) for:
- Complex reasoning tasks
- Explaining algorithms and concepts
- Research and analysis
- Mathematical computations
- Multi-step problem solving
- Comparing technologies/approaches
- Learning new concepts

### Use Kimi (via `route_to_model(task, "moonshot")`) for:
- Code review and quality checks
- Security vulnerability analysis
- Best practice recommendations
- Performance optimization suggestions
- Style and consistency checks
- PR review assistance

---

## When to Use Execution Tools

### Use `run_project` when:
- You need to verify code compiles/starts
- Testing after implementation
- Checking for runtime errors
- Verifying environment setup

### Use `test_project` when:
- Running automated tests
- Checking test coverage
- Verifying functionality
- After any code changes

### Use `analyze_errors` when:
- Understanding why something failed
- Before attempting fixes
- When errors are complex
- Needing root cause analysis

### Use `fix_issues` when:
- After error analysis
- Fixing common issues (dependencies, syntax)
- Applying AI-generated fixes
- Quick fixes for known patterns

### Use `verify_project` when:
- Multiple errors need fixing
- Want automatic fix-verify loop
- Starting a new project
- After major changes
- Preparing for deployment

### Use `orchestrate_full_development` when:
- Building a project from scratch
- Want end-to-end development
- Need planning through review

---

## The Verification Loop

The `verify_project` tool runs an intelligent loop:

```
┌─────────────┐
│    RUN      │ Execute project
└──────┬──────┘
       │
       ▼
┌─────────────┐
│    TEST     │ Run test suite
└──────┬──────┘
       │
       ▼
   ┌───────┐
   │Errors?│──NO──▶ ✅ SUCCESS
   └───┬───┘
       │YES
       ▼
┌─────────────┐
│  ANALYZE    │ Categorize errors
└──────┬──────┘
       │
       ▼
┌─────────────┐
│    FIX      │ Apply AI fixes
└──────┬──────┘
       │
       ▼
   ┌───────┐
   │Stuck? │──YES──▶ ⚠️ MANUAL NEEDED
   └───┬───┘
       │NO
       └────────▶ Back to RUN
```

### Trigger the verification loop when:
- Project has errors after implementation
- Tests are failing
- You want automatic error resolution
- Starting fresh development

### The loop stops when:
- All tests pass
- Max cycles reached (default 10)
- Same error occurs 3+ times
- No progress in 3 cycles

---

## How to Handle Auto-Fix Suggestions

### High Confidence Fixes (0.9+)
Auto-applied safely:
- Dependency installation
- Port conflicts
- Permission fixes
- Environment setup

### Medium Confidence Fixes (0.7-0.9)
Applied with backup:
- Syntax corrections
- Import fixes
- Simple code changes

### Low Confidence Fixes (< 0.7)
Review before applying:
- Logic changes
- Algorithm modifications
- Architecture updates

### Handling Fix Failures

If a fix doesn't work:
1. Backups are in `.backups/` directory
2. Rollback: `cp .backups/file.py.timestamp original.py`
3. Try manual fix with orchestrator guidance

---

## Full Development Cycle Guidelines

### Phase 1: Architecture (ChatGPT)
```
@ai-orchestrator orchestrate_task("Design architecture for [project description]")
```
- Get high-level design
- Define components and interfaces
- Plan database schema

### Phase 2: Implementation (Claude)
```
@ai-orchestrator orchestrate_task("Implement [feature] based on the architecture")
```
- Generate code
- Add error handling
- Include validation

### Phase 3: Execution & Testing
```
@ai-orchestrator verify_project("/path/to/project")
```
- Run and test project
- Auto-fix issues
- Loop until passing

### Phase 4: Review (Kimi)
```
@ai-orchestrator orchestrate_task("Review [project] for security and best practices")
```
- Security audit
- Performance check
- Code quality review

---

## Best Practices

### DO:
- Use `analyze_errors()` before `fix_issues()`
- Use `verify_project()` for complex issues
- Review low-confidence fixes manually
- Check backups after major fixes
- Run tests after every change
- Use specific task descriptions

### DON'T:
- Apply fixes without analysis for complex errors
- Ignore low-confidence warnings
- Skip the review phase
- Force fixes on production code
- Run `verify_project` infinitely (use limits)

---

## Task Description Templates

### For Architecture Tasks:
```
Design [component/system] that:
- Requirements: [list key requirements]
- Constraints: [list constraints]
- Integration: [how it connects to existing systems]
- Scale: [expected scale/usage]
```

### For Coding Tasks:
```
Implement [feature/function] that:
- Purpose: [what it should do]
- Inputs: [expected inputs]
- Outputs: [expected outputs]
- Edge cases: [known edge cases]
- Context: [relevant existing code]
```

### For Debugging Tasks:
```
Debug and fix:
- Error: [paste error message]
- Context: [what you were doing]
- Expected: [what should happen]
- Actual: [what happened]
```

### For Review Tasks:
```
Review this code for:
- [specific concerns: security, performance, style]
- Context: [what the code does]
- Focus areas: [priority review points]

[paste code]
```

---

## Quick Reference

### Task Routing
| Task Type | Primary Model | Tool Call |
|-----------|--------------|----------|
| Design system | ChatGPT | `route_to_model(task, "openai")` |
| Write code | Claude | `route_to_model(task, "anthropic")` |
| Debug errors | Claude | `route_to_model(task, "anthropic")` |
| Explain concepts | Gemini | `route_to_model(task, "gemini")` |
| Review code | Kimi | `route_to_model(task, "moonshot")` |
| Multi-step task | Auto | `orchestrate_task(task)` |

### Execution Tools
| Need | Tool | Command |
|------|------|---------|
| Run project | `run_project` | `run_project("/path")` |
| Run tests | `test_project` | `test_project("/path")` |
| Analyze errors | `analyze_errors` | `analyze_errors("/path")` |
| Fix issues | `fix_issues` | `fix_issues("/path")` |
| Full loop | `verify_project` | `verify_project("/path")` |

---

## Example Workflows

### New Feature Development
```
1. orchestrate_task("Design the feature architecture")
2. orchestrate_task("Implement the feature")
3. verify_project("/path/to/project")
4. orchestrate_task("Review the implementation")
```

### Fix Failing Tests
```
1. test_project("/path/to/project")  # See failures
2. analyze_errors("/path/to/project")  # Understand why
3. fix_issues("/path/to/project")  # Apply fixes
4. test_project("/path/to/project")  # Verify fixed
```

### Production Bug Fix
```
1. analyze_errors("/path/to/project", error="[paste error]")
2. route_to_model("Explain why this error occurs", "gemini")
3. fix_issues("/path/to/project")
4. verify_project("/path/to/project")
5. route_to_model("Review the fix for production safety", "moonshot")
```

---

## Error Handling

### If a model is unavailable:
- Orchestrator attempts alternative model
- Returns error with guidance
- Use `check_status()` to see available models

### If execution fails:
- Error output captured in result
- Use `analyze_errors()` for deep analysis
- Backups created before any fixes

### If stuck in verification loop:
- Loop auto-terminates after max cycles
- Report shows what couldn't be fixed
- Suggests manual intervention steps
